# –∏–º–ø–æ—Ä—Ç—ã
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

from utils import (
    make_splitter, 
    load_data_from_url,
    clean_wikipedia_text,
)

# —Å—Ö–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
CONFIGS = [
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    {"name": "sparse_optimized", "chunk_size": 300, "chunk_overlap": 30, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è BM25"},
    {"name": "dense_optimized", "chunk_size": 800, "chunk_overlap": 100, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"}, 
    {"name": "hybrid_balanced", "chunk_size": 500, "chunk_overlap": 50, "note": "–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"},
]

SRC_URL = "https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D0%B4%D0%BE%D0%BD%D0%B5%D0%B7%D0%B8%D1%8F"
QUESTIONS = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ –ò–Ω–¥–æ–Ω–µ–∑–∏—è?",
    "–ö–∞–∫–∞—è –ø–ª–æ—â–∞–¥—å –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–æ–µ –Ω–∞—Å–µ–ª–µ–Ω–∏–µ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–∞—è —Ä–µ–ª–∏–≥–∏—è –≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–∏–µ —è–∑—ã–∫–∏ –≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–∏–µ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–∏–µ –≥–æ—Ä–æ–¥–∞ –≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",
    "–ö–∞–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞ –≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",    
    "–ö–∞–∫–∏–µ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏?",    
]



def run_tests(embedding_model, configs, docs, questions):
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    dbs = []
    for cfg in configs:
        splitter = make_splitter(cfg)
        chunks = []
        for doc in docs:
            for chunk_text in splitter.split_text(doc.page_content):
                md = (doc.metadata or {}).copy() if hasattr(doc, "metadata") else {}
                chunks.append(Document(page_content=chunk_text, metadata=md))

        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –ë–î –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {cfg['name']} "
              f"(chunk_size={cfg['chunk_size']}, overlap={cfg['chunk_overlap']}), "
              f"–≤—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤={len(chunks)}")

        db = FAISS.from_documents(chunks, embedding_model)
        dbs.append(db)

    print("\n" + "="*80)
    print("üöÄ –ù–ê–ß–ê–õ–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –í–û–ü–†–û–°–û–í")
    print("="*80 + "\n")

    for q in questions:
        print(f"üîç –í–æ–ø—Ä–æ—Å: {q}")
        print("-" * 40)

        results_per_config = []
        for i, cfg in enumerate(configs):
            k = cfg.get("k", 2)
            docs_and_scores = dbs[i].similarity_search_with_score(q, k=k)
            scores = [score for _, score in docs_and_scores]
            avg_score = sum(scores) / len(scores) if scores else float('inf')
            results_per_config.append({
                'config': cfg,
                'scores': scores,
                'docs_and_scores': docs_and_scores,
                'avg_score': avg_score
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —Å–∫–æ—Ä—É (–Ω–∏–∂–Ω–∏–π —Å–∫–æ—Ä - –ª—É—á—à–µ)
        sorted_results = sorted(results_per_config, key=lambda x: x['avg_score'])

        # –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
        best = sorted_results[0]
        print(f"üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best['config']['name']} (avg_score={best['avg_score']:.4f})")
        if best['docs_and_scores']:
            snippet = best['docs_and_scores'][0][0].page_content[:300].replace("\n", " ")
            print(f"   üìÑ –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {snippet}...")

        # –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
        worst = sorted_results[-1]
        print(f"üëé –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {worst['config']['name']} (avg_score={worst['avg_score']:.4f})")
        if worst['docs_and_scores']:
            snippet = worst['docs_and_scores'][0][0].page_content[:300].replace("\n", " ")
            print(f"   üìÑ –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {snippet}...")

        print("\n" + "-"*60 + "\n")

def main():
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    docs = load_data_from_url(SRC_URL)
    docs_cleaned = [Document(page_content=clean_wikipedia_text(doc.page_content), metadata=doc.metadata) for doc in docs]
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    embedding_model = HuggingFaceEmbeddings(model_name="cointegrated/rubert-tiny2")
    print("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤...")
    run_tests(embedding_model, CONFIGS, docs_cleaned, QUESTIONS)

if __name__ == "__main__":
    main()
