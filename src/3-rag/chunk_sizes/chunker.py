# –∏–º–ø–æ—Ä—Ç—ã
import os
import click
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

from utils import (
    make_splitter, 
    load_data_from_url,
    clean_wikipedia_text,
)
from llm_assessor import LLMAssessor
from evaluators import ScoreBasedEvaluator, LLMBasedEvaluator

# —Å—Ö–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
CONFIGS = [
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    {"name": "sparse_optimized", "chunk_size": 300, "chunk_overlap": 30, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è BM25"},
    {"name": "dense_optimized", "chunk_size": 800, "chunk_overlap": 100, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"}, 
    {"name": "hybrid_balanced", "chunk_size": 500, "chunk_overlap": 50, "note": "–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"},
]

SRC_URL = "https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D0%B4%D0%BE%D0%BD%D0%B5%D0%B7%D0%B8%D1%8F"
QUESTIONS = [
    "—Å—Ç–æ–ª–∏—Ü–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–Ω–∞—Å–µ–ª–µ–Ω–∏–µ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∫–∞–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞ –≤—Ö–æ–¥—è—Ç –≤ —Å–æ—Å—Ç–∞–≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∏—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–æ–∑–≥–ª–∞—à–µ–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "—Ä–µ–ª–∏–≥–∏–æ–∑–Ω—ã–π —Å–æ—Å—Ç–∞–≤ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–≤—É–ª–∫–∞–Ω—ã –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ö—Ä–∞–∫–∞—Ç–∞—É –∏–∑–≤–µ—Ä–∂–µ–Ω–∏–µ",
    "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–∏–π —è–∑—ã–∫ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π",
    "—è–≤–∞–Ω—Ü—ã –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞—Ä–æ–¥ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ø–µ—Ä–∏–æ–¥ –ù–æ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –°—É—Ö–∞—Ä—Ç–æ",
    "–±—É–¥–¥–∏—Å—Ç—Å–∫–∏–π —Ö—Ä–∞–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞",
    "–î–≤–∏–∂–µ–Ω–∏–µ 30 —Å–µ–Ω—Ç—è–±—Ä—è 1965 –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç",
    "–æ–ª–∏–º–ø–∏–π—Å–∫–∏–µ –º–µ–¥–∞–ª–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–±–∞–¥–º–∏–Ω—Ç–æ–Ω",
]


def output_sample_text(text, max_len_of_sample):
    snippet = text[:max_len_of_sample].replace("\n", " ")
    is_cut = len(text) > max_len_of_sample
    print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {snippet}{"..." if is_cut else ""}")


def run_tests(
        embedding_model, 
        configs, 
        docs, 
        questions, 
        llm_model,
        api_key,
        max_len_of_sample=500,
        evaluation_mode="score-based",
):
    """
    Run tests with different evaluation modes
    
    Args:
        embedding_model: Embedding model for vector search
        configs: List of chunking configurations
        docs: List of documents to chunk
        questions: List of test questions
        max_len_of_sample: Max length for sample output
        evaluation_mode: "score-based" or "llm-based"
        llm_model: Model name for LLM assessment (optional)
    """
    # Validate evaluation mode
    if evaluation_mode not in ["score-based", "llm-based"]:
        raise ValueError(f"Invalid evaluation_mode: {evaluation_mode}")
    
    # Initialize evaluator based on mode
    if evaluation_mode == "llm-based":
        assessor = LLMAssessor(model_name=llm_model, api_key=api_key)
        evaluator = LLMBasedEvaluator(assessor)
        print(f"‚úÖ LLM evaluator initialized")
    else:
        evaluator = ScoreBasedEvaluator()
        print(f"‚úÖ Score-based evaluator initialized")
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    dbs = []
    for cfg in configs:
        splitter = make_splitter(cfg)
        chunks = []
        for doc in docs:
            for chunk_text in splitter.split_text(doc.page_content):
                md = (doc.metadata or {}).copy() if hasattr(doc, "metadata") else {}
                chunks.append(Document(page_content=chunk_text, metadata=md))

        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –ë–î –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {cfg['name']} "
              f"(chunk_size={cfg['chunk_size']}, overlap={cfg['chunk_overlap']}), "
              f"–≤—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤={len(chunks)}")

        db = FAISS.from_documents(chunks, embedding_model)
        dbs.append(db)

    print("\n" + "="*80)
    print("üöÄ –ù–ê–ß–ê–õ–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –í–û–ü–†–û–°–û–í")
    print("="*80 + "\n")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–±–µ–¥ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    win_stats = {cfg['name']: 0 for cfg in configs}

    for q in questions:
        print(f"üîç –í–æ–ø—Ä–æ—Å: {q}")
        print(f"üìä Evaluation Mode: {evaluation_mode}")
        print("-" * 40)

        # Use evaluator to get results
        k = configs[0].get("k", 2)
        sorted_results = evaluator.evaluate(dbs, configs, q, k=k)
        
        # Update winner tracking and output
        best = sorted_results[0]
        config_name = best['config']['name']
        win_stats[config_name] += 1
        
        # Output results based on mode
        if evaluation_mode == "score-based":
            print(f"üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_name} (avg_score={best['avg_score']:.4f})")
            if best.get('docs_and_scores'):
                original_text = best['docs_and_scores'][0][0].page_content
                output_sample_text(original_text, max_len_of_sample)
        elif evaluation_mode == "llm-based":
            print(f"üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_name} (llm_score={best['llm_score']}/100)")
            print(f"   üí° Reasoning: {best['reasoning']}")
            if best.get('doc'):
                output_sample_text(best['doc'].page_content, max_len_of_sample)
        
        # Show worst configuration
        worst = sorted_results[-1]
        worst_config_name = worst['config']['name']
        if evaluation_mode == "score-based":
            print(f"üëé –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {worst_config_name} (avg_score={worst['avg_score']:.4f})")
            if worst.get('docs_and_scores'):
                original_text = worst['docs_and_scores'][0][0].page_content
                output_sample_text(original_text, max_len_of_sample)
        elif evaluation_mode == "llm-based":
            print(f"üëé –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {worst_config_name} (llm_score={worst['llm_score']}/100)")
            print(f"   üí° Reasoning: {worst['reasoning']}")
            if worst.get('doc'):
                output_sample_text(worst['doc'].page_content, max_len_of_sample)
        
        print("\n" + "-"*60 + "\n")

    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("\n" + "="*80)
    print("üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–ë–ï–î")
    print("="*80 + "\n")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ–±–µ–¥
    sorted_stats = sorted(win_stats.items(), key=lambda x: x[1], reverse=True)
    
    total_questions = len(questions)
    for rank, (config_name, wins) in enumerate(sorted_stats, 1):
        percentage = (wins / total_questions) * 100
        bar_length = int(percentage / 2)  # –ú–∞—Å—à—Ç–∞–± –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        bar = "‚ñà" * bar_length
        print(f"{rank}. {config_name:20s} | {wins:2d}/{total_questions} –ø–æ–±–µ–¥ ({percentage:5.1f}%) {bar}")
    
    print("\n" + "="*80)
    winner = sorted_stats[0]
    print(f"üéâ –ü–û–ë–ï–î–ò–¢–ï–õ–¨: {winner[0]} —Å {winner[1]} –ø–æ–±–µ–¥–∞–º–∏ –∏–∑ {total_questions} –≤–æ–ø—Ä–æ—Å–æ–≤!")
    print("="*80 + "\n")

@click.command()
@click.option(
    '--eval-mode',
    type=click.Choice(['score-based', 'llm-based']),
    default='score-based',
    help='Evaluation mode for chunk quality'
)
def main(eval_mode):
    """Evaluate RAG chunking strategies"""
    # Load environment
    load_dotenv()
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    docs = load_data_from_url(SRC_URL)
    docs_cleaned = [
        Document(
            page_content=clean_wikipedia_text(doc.page_content), 
            metadata=doc.metadata
        ) 
        for doc in docs
    ]
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    embedding_model = HuggingFaceEmbeddings(model_name="cointegrated/rubert-tiny2")

    llm_model = os.getenv("OPENROUTER_API_MODEL", "x-ai/grok-4-fast")
    api_key = os.getenv("OPENROUTER_API_KEY")
    
    print(f"–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ (—Ä–µ–∂–∏–º: {eval_mode})...")
    run_tests(
        embedding_model, 
        CONFIGS, 
        docs_cleaned, 
        QUESTIONS,
        llm_model=llm_model,
        api_key=api_key,
        evaluation_mode=eval_mode,
    )

if __name__ == "__main__":
    main()
