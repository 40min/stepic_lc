# –∏–º–ø–æ—Ä—Ç—ã
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

from utils import (
    make_splitter, 
    load_data_from_url,
    clean_wikipedia_text,
)

# —Å—Ö–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
CONFIGS = [
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    {"name": "sparse_optimized", "chunk_size": 300, "chunk_overlap": 30, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è BM25"},
    {"name": "dense_optimized", "chunk_size": 800, "chunk_overlap": 100, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"}, 
    {"name": "hybrid_balanced", "chunk_size": 500, "chunk_overlap": 50, "note": "–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"},
]

SRC_URL = "https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D0%B4%D0%BE%D0%BD%D0%B5%D0%B7%D0%B8%D1%8F"
QUESTIONS = [
    "—Å—Ç–æ–ª–∏—Ü–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–Ω–∞—Å–µ–ª–µ–Ω–∏–µ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∫–∞–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞ –≤—Ö–æ–¥—è—Ç –≤ —Å–æ—Å—Ç–∞–≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∏—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–æ–∑–≥–ª–∞—à–µ–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "—Ä–µ–ª–∏–≥–∏–æ–∑–Ω—ã–π —Å–æ—Å—Ç–∞–≤ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–≤—É–ª–∫–∞–Ω—ã –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ö—Ä–∞–∫–∞—Ç–∞—É –∏–∑–≤–µ—Ä–∂–µ–Ω–∏–µ",
    "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–∏–π —è–∑—ã–∫ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π",
    "—è–≤–∞–Ω—Ü—ã –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞—Ä–æ–¥ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ø–µ—Ä–∏–æ–¥ –ù–æ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –°—É—Ö–∞—Ä—Ç–æ",
    "–±—É–¥–¥–∏—Å—Ç—Å–∫–∏–π —Ö—Ä–∞–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞",
    "–î–≤–∏–∂–µ–Ω–∏–µ 30 —Å–µ–Ω—Ç—è–±—Ä—è 1965 –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç",
    "–æ–ª–∏–º–ø–∏–π—Å–∫–∏–µ –º–µ–¥–∞–ª–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–±–∞–¥–º–∏–Ω—Ç–æ–Ω",
]


def output_sample_text(text, max_len_of_sample):
    snippet = text[:max_len_of_sample].replace("\n", " ")
    is_cut = len(text) > max_len_of_sample
    print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {snippet}{"..." if is_cut else ""}")


def run_tests(embedding_model, configs, docs, questions, max_len_of_sample=500):
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    dbs = []
    for cfg in configs:
        splitter = make_splitter(cfg)
        chunks = []
        for doc in docs:
            for chunk_text in splitter.split_text(doc.page_content):
                md = (doc.metadata or {}).copy() if hasattr(doc, "metadata") else {}
                chunks.append(Document(page_content=chunk_text, metadata=md))

        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –ë–î –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {cfg['name']} "
              f"(chunk_size={cfg['chunk_size']}, overlap={cfg['chunk_overlap']}), "
              f"–≤—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤={len(chunks)}")

        db = FAISS.from_documents(chunks, embedding_model)
        dbs.append(db)

    print("\n" + "="*80)
    print("üöÄ –ù–ê–ß–ê–õ–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –í–û–ü–†–û–°–û–í")
    print("="*80 + "\n")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–±–µ–¥ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    win_stats = {cfg['name']: 0 for cfg in configs}

    for q in questions:
        print(f"üîç –í–æ–ø—Ä–æ—Å: {q}")
        print("-" * 40)

        results_per_config = []
        for i, cfg in enumerate(configs):
            k = cfg.get("k", 2)
            docs_and_scores = dbs[i].similarity_search_with_score(q, k=k)
            scores = [score for _, score in docs_and_scores]
            avg_score = sum(scores) / len(scores) if scores else float('inf')
            results_per_config.append({
                'config': cfg,
                'scores': scores,
                'docs_and_scores': docs_and_scores,
                'avg_score': avg_score
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —Å–∫–æ—Ä—É (–Ω–∏–∑–∫–∏–π —Å–∫–æ—Ä - –ª—É—á—à–µ –¥–ª—è FAISS distance)
        sorted_results = sorted(results_per_config, key=lambda x: x['avg_score'])

        # –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
        best = sorted_results[0]
        win_stats[best['config']['name']] += 1
        
        print(f"üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best['config']['name']} (avg_score={best['avg_score']:.4f})")
        if best['docs_and_scores']:            
            original_text = best['docs_and_scores'][0][0].page_content            
            output_sample_text(original_text, max_len_of_sample)

        # –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
        worst = sorted_results[-1]
        print(f"üëé –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {worst['config']['name']} (avg_score={worst['avg_score']:.4f})")
        if worst['docs_and_scores']:
            original_text = worst['docs_and_scores'][0][0].page_content            
            output_sample_text(original_text, max_len_of_sample)            

        print("\n" + "-"*60 + "\n")

    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("\n" + "="*80)
    print("üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–ë–ï–î")
    print("="*80 + "\n")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ–±–µ–¥
    sorted_stats = sorted(win_stats.items(), key=lambda x: x[1], reverse=True)
    
    total_questions = len(questions)
    for rank, (config_name, wins) in enumerate(sorted_stats, 1):
        percentage = (wins / total_questions) * 100
        bar_length = int(percentage / 2)  # –ú–∞—Å—à—Ç–∞–± –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        bar = "‚ñà" * bar_length
        print(f"{rank}. {config_name:20s} | {wins:2d}/{total_questions} –ø–æ–±–µ–¥ ({percentage:5.1f}%) {bar}")
    
    print("\n" + "="*80)
    winner = sorted_stats[0]
    print(f"üéâ –ü–û–ë–ï–î–ò–¢–ï–õ–¨: {winner[0]} —Å {winner[1]} –ø–æ–±–µ–¥–∞–º–∏ –∏–∑ {total_questions} –≤–æ–ø—Ä–æ—Å–æ–≤!")
    print("="*80 + "\n")

def main():
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    docs = load_data_from_url(SRC_URL)
    docs_cleaned = [Document(page_content=clean_wikipedia_text(doc.page_content), metadata=doc.metadata) for doc in docs]
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    embedding_model = HuggingFaceEmbeddings(model_name="cointegrated/rubert-tiny2")
    print("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤...")
    run_tests(embedding_model, CONFIGS, docs_cleaned, QUESTIONS)

if __name__ == "__main__":
    main()
