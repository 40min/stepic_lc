# –∏–º–ø–æ—Ä—Ç—ã
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

from utils import (
    make_splitter, 
    load_data_from_url,
    clean_wikipedia_text,
)

# —Å—Ö–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
CONFIGS = [
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    {"name": "sparse_optimized", "chunk_size": 300, "chunk_overlap": 30, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è BM25"},
    {"name": "dense_optimized", "chunk_size": 800, "chunk_overlap": 100, "note": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"}, 
    {"name": "hybrid_balanced", "chunk_size": 500, "chunk_overlap": 50, "note": "–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"},
]

SRC_URL = "https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D0%B4%D0%BE%D0%BD%D0%B5%D0%B7%D0%B8%D1%8F"
QUESTIONS = [
    "—Å—Ç–æ–ª–∏—Ü–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ –∏ –ø–ª–∞–Ω—ã –ø–µ—Ä–µ–Ω–æ—Å–∞",
    "–Ω–∞—Å–µ–ª–µ–Ω–∏–µ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ –ø–æ –ø–µ—Ä–µ–ø–∏—Å–∏ 2020 –≥–æ–¥–∞",
    "–∫–∞–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞ –≤—Ö–æ–¥—è—Ç –≤ —Å–æ—Å—Ç–∞–≤ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–∏—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–æ–∑–≥–ª–∞—à–µ–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ 1945",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ –ü—Ä–∞–±–æ–≤–æ –°—É–±–∏–∞–Ω—Ç–æ",
    "—Ä–µ–ª–∏–≥–∏–æ–∑–Ω—ã–π —Å–æ—Å—Ç–∞–≤ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ –∏—Å–ª–∞–º",
    "–≤—É–ª–∫–∞–Ω—ã –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ –ö—Ä–∞–∫–∞—Ç–∞—É –∏–∑–≤–µ—Ä–∂–µ–Ω–∏–µ",
    "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏ –í–í–ü 2023",
    "–∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–∏–π —è–∑—ã–∫ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π",
    "—è–≤–∞–Ω—Ü—ã –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞—Ä–æ–¥ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏",
    "–ø–µ—Ä–∏–æ–¥ –ù–æ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –°—É—Ö–∞—Ä—Ç–æ",
    "–ë–æ—Ä–æ–±—É–¥—É—Ä –±—É–¥–¥–∏—Å—Ç—Å–∫–∏–π —Ö—Ä–∞–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞",
    "–î–≤–∏–∂–µ–Ω–∏–µ 30 —Å–µ–Ω—Ç—è–±—Ä—è 1965 –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç",
    "–±–∞–¥–º–∏–Ω—Ç–æ–Ω –æ–ª–∏–º–ø–∏–π—Å–∫–∏–µ –º–µ–¥–∞–ª–∏ –ò–Ω–¥–æ–Ω–µ–∑–∏–∏"
]


def run_tests(embedding_model, configs, docs, questions):
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    dbs = []
    for cfg in configs:
        splitter = make_splitter(cfg)
        chunks = []
        for doc in docs:
            for chunk_text in splitter.split_text(doc.page_content):
                md = (doc.metadata or {}).copy() if hasattr(doc, "metadata") else {}
                chunks.append(Document(page_content=chunk_text, metadata=md))

        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –ë–î –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {cfg['name']} "
              f"(chunk_size={cfg['chunk_size']}, overlap={cfg['chunk_overlap']}), "
              f"–≤—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤={len(chunks)}")

        db = FAISS.from_documents(chunks, embedding_model)
        dbs.append(db)

    print("\n" + "="*80)
    print("üöÄ –ù–ê–ß–ê–õ–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –í–û–ü–†–û–°–û–í")
    print("="*80 + "\n")

    for q in questions:
        print(f"üîç –í–æ–ø—Ä–æ—Å: {q}")
        print("-" * 40)

        results_per_config = []
        for i, cfg in enumerate(configs):
            k = cfg.get("k", 2)
            docs_and_scores = dbs[i].similarity_search_with_score(q, k=k)
            scores = [score for _, score in docs_and_scores]
            avg_score = sum(scores) / len(scores) if scores else float('inf')
            results_per_config.append({
                'config': cfg,
                'scores': scores,
                'docs_and_scores': docs_and_scores,
                'avg_score': avg_score
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —Å–∫–æ—Ä—É (–≤—ã—Å–æ–∫–∏–π —Å–∫–æ—Ä - –ª—É—á—à–µ)
        sorted_results = sorted(results_per_config, key=lambda x: x['avg_score'], reverse=True)

        # –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
        best = sorted_results[0]
        print(f"üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best['config']['name']} (avg_score={best['avg_score']:.4f})")
        if best['docs_and_scores']:
            snippet = best['docs_and_scores'][0][0].page_content[:300].replace("\n", " ")
            print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {snippet}...")

        # –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
        worst = sorted_results[-1]
        print(f"üëé –•—É–¥—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {worst['config']['name']} (avg_score={worst['avg_score']:.4f})")
        if worst['docs_and_scores']:
            original_text = worst['docs_and_scores'][0][0].page_content
            snippet = original_text[:300].replace("\n", " ")
            is_cut = len(original_text) > 300
            print(f"   üìÑ –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {snippet}{"..." if is_cut else ""}")

        print("\n" + "-"*60 + "\n")

def main():
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    docs = load_data_from_url(SRC_URL)
    docs_cleaned = [Document(page_content=clean_wikipedia_text(doc.page_content), metadata=doc.metadata) for doc in docs]
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    embedding_model = HuggingFaceEmbeddings(model_name="cointegrated/rubert-tiny2")
    print("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤...")
    run_tests(embedding_model, CONFIGS, docs_cleaned, QUESTIONS)

if __name__ == "__main__":
    main()
